# Made by: Airil / Ainhoa

import re
import os
import requests
import subprocess
import ipaddress
import pythonping
import signal
import socket
import xml.etree.ElementTree as ET
import whois
import psutil
import dns.query
import dns.resolver
import dns.zone

from termcolor import colored
from pwn import *
from multiprocessing.pool import ThreadPool
from bs4 import BeautifulSoup
from webtech import WebTech

DN = open(os.devnull, 'w')
lock = threading.Lock()
stop_brute_force = False

def handle_signal(signal, frame):
        print(colored("\n[+] Exiting and cleaning up...", 'green'))
        stop_brute_force = True
        sys.exit(0)

signal.signal(signal.SIGINT, handle_signal)

def simple_progress_update(status):
    print(f"Status: {status}", end='\r')

def get_whois_info(domain):
    try:
        w = whois.whois(domain)
        print(colored("\n DOMAIN INFORMATION:", 'green', attrs=['dark', 'underline']) +
                colored("\n  Domain Name:", 'green'), colored(w.domain_name, 'cyan') +
                colored("\n  Registrar:", 'green'), colored(w.registrar, 'cyan') +
                colored("\n  Creation Date:", 'green'), colored(w.creation_date, 'cyan') +
                colored("\n  Expiration Date:", 'green'), colored(w.expiration_date, 'cyan') +
                colored("\n  Updated Date:", 'green'), colored(w.updated_date, 'cyan') +
                colored("\n  Name Servers:", 'green'), colored(w.name_servers, 'cyan') +
                colored("\n  Status:", 'green'), colored(w.status, 'cyan') +
                colored("\n  Emails:", 'green'), colored(w.emails, 'cyan') +
                colored("\n  WHOIS Server:", 'green'), colored(w.whois_server, 'cyan') +
                colored("\n  Organization:", 'green'), colored(w.org, 'cyan') +
                colored("\n  Address:", 'green'), colored(w.address, 'cyan') +
                colored("\n  City:", 'green'), colored(w.city, 'cyan') +
                colored("\n  State:", 'green'), colored(w.state, 'cyan') +
                colored("\n  Country:", 'green'), colored(w.country, 'cyan') +
                colored("\n  Zip Code:", 'green'), colored(w.zipcode, 'cyan') +
                colored("\n  Phone:", 'green'), colored(w.phone, 'cyan'))
        
    except Exception as e:
        print(colored("\n[!]Error:", 'red'), colored(str(e), 'yellow'))

def check_port(host, port, protocolo):
    try:
        if protocolo == "tcp":
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        elif protocolo == "udp":
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        else:
            print(colored("\n[!] Invalid protocol", 'red'))
            return None
        
        s.settimeout(0.3)
        resultado = s.connect_ex((host, port)) if protocolo == "tcp" else s.sendto(b"", (host, port))
        
        if resultado == 0:
            print(colored("[+]", 'green') + " Open port founded: " + colored(port, 'cyan'))
            
            try:
                if protocolo == "tcp":
                    # Fragmenta los paquetes TCP
                    s.sendall(b"GET / HTTP/1.1\r\n")
                    s.sendall(b"Host: example.com\r\n")
                    s.sendall(b"\r\n")

                    banner = s.recv(1024)
                    print(f"Banner: {banner.decode().strip()}")
                
                service = socket.getservbyport(port, protocolo)
                if service:
                    return service
                else:
                    return "uwu"
            except:
                return "uwu"
                   
        # Cierra el socket para que haga SYN --> SYN ACK --> RST en vez de ACK
        s.close()
        
    except:
        print(colored("\n[!] Error to connect"))

    return None

def port_scan(host, ports, protocolo):
    print("\n")
    common_ports = [80, 443, 21, 22, 25, 23, 3389, 110, 445, 139, 143, 53, 135, 3306, 8080, 1723, 111, 995, 993, 5900, 1025, 587, 8888, 199, 1720, 465, 20, 115, 6660, 6669, 8443, 554, 3306, 4949, 1352, 1433, 3268, 5631, 5632, 7070, 3690, 79, 6699, 6698, 873, 2082, 2083, 6001, 6002, 989, 990]
    valid_ports = {}

    signal.signal(signal.SIGINT, handle_signal)

    if ports == "all":
        start_port, end_port = 1, 65535
    elif ports == "":
        start_port, end_port = 1, 2
    else:
        start_port, end_port = map(int, ports.split())

    for port in common_ports:
        simple_progress_update(f"Scanning port {port}")
        service = check_port(host, port, protocolo)
        if service:
            valid_ports[port] = service

    if start_port >= 1:
        simple_progress_update(f"Scanning port {port}")
        for port in range(start_port, end_port + 1):
            if port not in common_ports:
                service = check_port(host, port, protocolo)
                if service:
                    valid_ports[port] = service
    
    return valid_ports

def scan_local_network(ipandcidr):
    print(colored("[+] Scanning localnet...", 'green'))
    network = ipaddress.ip_network(ipandcidr, strict=False)
    active_hosts = []

    for host in network.hosts():
        ip = str(host)
        try:
            response = pythonping.ping(ip, count=1, timeout=0.1)
            if response.success():
                active_hosts.append(ip)
        except Exception as e:
            print(colored(f"[!] Error pinging {ip}: {e}", 'red'))
    return active_hosts

def check_subdomain(args):
    subdomain, domain, firsturl = args
    brute_force_subdomain = subdomain + '.' + domain
    url = firsturl + brute_force_subdomain
    try:
        sys.stdout.write(f"\rChecking: {subdomain}{' ' * 20}")
        sys.stdout.flush()
        resp = requests.get(str(url))
        if resp.status_code < 400:
            print(colored("[+] Subdomain found: ", 'green') + colored(subdomain, 'blue'))
            return subdomain
    except:
        pass
    return ""

def subdomains(domain, brute, sslcert):
    subdomains = []
    # Abusando de la transparencia del ssl
    print(colored("[+] Searching for subdomains by leveraging SSL transparency (pasive method)...", 'green'))
    target = re.sub('.*www\.', '', domain, 1).split('/')[0].strip()
    if sslcert == "y" or sslcert == "yes":
        req = requests.get("https://crt.sh/?q=%.{d}&output=json".format(d=target))
    else:
        req = requests.get("http://crt.sh/?q=%.{d}&output=json".format(d=target))

    if req.status_code >= 400:
        print(colored("[!] ERROR: domain not found! check if its correct, internet conexion and restart this tool please", 'red'))
        return subdomains

    for (key, value) in enumerate(req.json()):
        subdomains.append(value['name_value'])
    
    # Con OSINT
    print(colored("[+] Searching for more subdomains with OSINT (pasive method)...", 'green'))
    
    # Falta por hacer esto, que pereza
    
    # Domain Zone Transfer
    print(colored("[+] Attempting Domain Zone Transfer for search more subdomains...", 'green'))
    try:
        # Obtener los servidores NS (nameservers) para el dominio
        ns_answer = dns.resolver.resolve(domain, 'NS')
        for ns in ns_answer:
            # Y aqui se intenta la transferencia de zona para intentar conseguir mas subdominios existentes
            try:
                zone = dns.zone.from_xfr(dns.query.xfr(str(ns), domain))
                names = zone.nodes.keys()
                for n in names:
                    subdomain = str(n) + '.' + domain
                    if subdomain not in subdomains:
                        subdomains.append(subdomain)
                print(colored(f"[+] Transfer successful from {ns}. Subdomains found:", 'green'))
                for subdomain in subdomains:
                    print(colored(subdomain, 'cyan'))
                return subdomains
            except Exception as e:
                print(colored(f"[-] Transfer from {ns} failed: {e}", 'red'))
    except Exception as e:
        print(colored(f"[!] Error resolving NS records for {domain}: {e}", 'red'))
    
    # Bruteforce
    signal.signal(signal.SIGINT, handle_signal)

    if sslcert == "y" or sslcert == "yes":
        firsturl = "https://"
    else:
        firsturl = "http://"

    if brute == "y" or brute == "yes":
        print(colored("[+] Starting fuzzing for subdomains...", 'green'))
        dictionary_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'subdomain.txt')
        with open(dictionary_file) as f:
            dictionary = f.read().splitlines()

        args = [(subdomain, domain, firsturl) for subdomain in dictionary]
        with ThreadPool(processes=100) as pool:
            for result in pool.imap_unordered(check_subdomain, args):
                if result != "":
                    subdomains.append(result)
                with lock:
                    if stop_brute_force:
                        pool.terminate()
                        break

        print(colored("[+] Brute force finished!", 'green'))

    subdomains = sorted(set(subdomains))
    return subdomains

def extract_directories(url, content): # para sacar los directorios de robots.txt y sitemap.xml
    pattern = r"\/[\w\/.-]+\/"
    directories = re.findall(pattern, content)
    
    processed_directories = []
    for directory in directories:
        if directory.startswith(url):
            directory = directory[len(url):]
        
        if not directory.startswith("/"):
            directory = "/" + directory
        
        processed_directories.append(directory)
    
    return processed_directories

def check_url(args):
    url, word, hc = args
    word = word.strip()
    try:
        resp = requests.get(url + word, allow_redirects=False)
        
        if int(resp.status_code) != 404 and int(resp.status_code) not in hc:
            print(colored("[+] Directory found: ", 'green') + colored("/", 'cyan') + colored(word, 'cyan') + colored("  -->  Status code: ", 'green') + colored(resp.status_code, 'cyan'))
            return word
        else:
            return ""
    except:
        return ""

def dir_fuzzing(web, hc):
    print(colored("\n[+] Finding directories...\n", 'green'))
    directories = []
    
    # PASIVE
    robots_url = web + "/robots.txt"
    response = requests.get(robots_url)
    if response.status_code == 200:
        print(colored("[+]", 'green') + colored(" robots.txt ", 'cyan') + colored("found!", 'green'))
        directories += extract_directories(web, response.text)
        
    sitemap_url = web + "/sitemap.xml"
    response = requests.get(sitemap_url)
    if response.status_code == 200:
        print(colored("[+]", 'green') + colored(" sitemap.xml ", 'cyan') + colored("found!", 'green'))
        directories += extract_directories(web, response.text)
    
    # ACTIVE
    signal.signal(signal.SIGINT, handle_signal)
    
    print(colored("\n[+] Strarting Fuzzing\n", 'green'))
    if web[-1] != '/':
        web = web+"/"

    dictionary_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'directory_list.txt')
    with open(dictionary_file, "r") as f:
        args = [(web, word, hc) for word in f]
        with ThreadPool(processes=100) as pool:
            for result in pool.imap_unordered(check_url, args):
                if result != "":
                    directories.append(result)
                if stop_brute_force:
                    with lock:
                        pool.terminate()
                        break
    
    print(colored("[+] Fuzzing finished!", 'green'))
    
    processed_directories = []
    for directory in directories:
        if directory.startswith("/" + web[8:]):
            continue
        if directory.startswith("//" + web[8:]):
            continue
        processed_directories.append(directory)
    
    processed_directories = set(processed_directories)
    
    return processed_directories

def check_url2(args):
    url, word, hc = args
    word = word.strip()
    try:
        resp = requests.get(url + word, allow_redirects=False)
        
        if int(resp.status_code) != 404 and str(resp.status_code) not in hc:
            return word
        else:
            return ""
    except:
        return ""

def directory_map(url, thread_num=20, hc=None, path=0, wordlist="directory_list.txt"):
    extensions = ['.js', '.php', '.html', '.txt']
    signal.signal(signal.SIGINT, handle_signal)

    wordlist = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'directory_list.txt')

    if path == 0:
        print(colored("\nDIRECTORY MAP:", 'green', attrs=['dark', 'underline']))
    
    url = url if url[-1] == '/' else url + '/'
    
    directories = []
    with open(wordlist, "r") as f:
        args = [(url, word, hc) for word in f]
        args2 = []
        
        if path == 0:
            dictionary_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'reducted_dirs.txt')
            with open(dictionary_file, "r") as f:
                ags = [(url, word, hc) for word in f]
                for i in ags:
                    args2.append((i[0], i[1].strip(), i[2]))
                    for a in extensions:
                        args2.append((i[0], str(i[1].strip()+a), i[2]))
        
        for i in args:
            args2.append((i[0], i[1].strip(), i[2]))
            for a in extensions:
                args2.append((i[0], str(i[1].strip()+a), i[2]))

        with ThreadPool(processes=int(thread_num)) as pool:
            for result in pool.imap_unordered(check_url2, args2):
                if result != "":
                    directories.append(result)
                    espacio = " " * 4 * path # y tiempo, no te jode XD
                    codigolioco = requests.get(url + result, allow_redirects=False).status_code

                    if path == 0:
                        print("\n/" + str(result) + colored(str(" --> " + str(codigolioco)), 'cyan'))
                    else:
                        print(espacio + "/" + str(result) + colored(str(" --> " + str(codigolioco)), 'cyan'))

                    if "." not in result:
                        directory_map(url + result, thread_num, hc, path+1, wordlist="reducted_dirs.txt")
                
                if stop_brute_force:
                    with lock:
                        pool.terminate()
                        break

    print(colored("\n[+] Fuzzing finished!", 'green'))

def check_cve(tech, version):
    def request_get(url, params=None):
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            print(f"Request error: {e}")
            return None

    def find_cpes(component, version):
        base_url = "https://nvd.nist.gov/products/cpe/search/results"
        params = {"namingFormat": "2.3", "keyword": f"{component} {version}"}
        response = request_get(base_url, params)
        if response:
            return re.findall(r'cpe:2\.3:(.*?)<', response.text)
        return []

    def fetch_cve_details(cpe_string):
        base_url = "https://services.nvd.nist.gov/rest/json/cves/1.0"
        params = {"cpeMatchString": cpe_string, "resultsPerPage": "2"}  # Limit to 2 for brevity
        response = request_get(base_url, params)
        if response:
            return response.json().get('result', {}).get('CVE_Items', [])
        return []

    def fetch_github_urls(cve_id):
        api_url = f"https://poc-in-github.motikan2010.net/api/v1/?cve_id={cve_id}"
        response = request_get(api_url)
        if response and response.json().get("pocs"):
            return [poc["html_url"] for poc in response.json()["pocs"]]
        return []

    def search_packetstorm_info(product_name):
        url = f"https://packetstormsecurity.com/search/?q={product_name}&s=files"
        response = request_get(url)
        if response:
            soup = BeautifulSoup(response.text, 'html.parser')
            return [f"https://packetstormsecurity.com{a['href']}" for a in soup.find_all('a', href=True) if '/files/download/' in a['href']]
        return []

    cpe_strings = find_cpes(tech, version)
    if cpe_strings:
        for cpe in cpe_strings:
            print(f"Found CPE: {cpe}")
            cve_details = fetch_cve_details(cpe)
            for cve in cve_details:
                cve_id = cve['cve']['CVE_data_meta']['ID']
                print(f"Found CVE: {cve_id}")
                github_urls = fetch_github_urls(cve_id)
                for url in github_urls:
                    print(f"GitHub URL: {url}")
    else:
        print("No CPEs found.")

    download_links = search_packetstorm_info(tech)
    for link in download_links:
        print(f"Download Link: {link}")

def tec_enum(url):
    print(colored("\nWEB TECHNOLOGIES:\n", 'green', attrs=['dark', 'underline']))

    try:
        wt = WebTech()
        results = wt.start_from_url(url, timeout=1)

        lines = results.split('\n')
        technology_lines = [line.strip() for line in lines if '-' in line and 'Detected technologies:' in lines[:lines.index(line)]]
        technologies = [line.replace('-', '').strip() for line in technology_lines]


        if technologies != []:
            for technology in technologies:
                print(colored("· ", 'green'), colored(technology, 'cyan'))
                
                match = re.match(r"([a-zA-Z\s]+)(\s\d+\.\d+(\.\d+)?)?", technology)
                if match:
                    technology = match.group(1).strip()
                    version = match.group(2) if match.group(2) else 'uwu'

                if version != 'uwu':
                    check_cve(technology, version)

    except Exception as e:
        print(f"Error using WebTech: {e}")

def webdav_enum(url, depth=1, username=None, password=None):
    try:
        auth = None
        if username and password:
            auth = (username, password)

        try:
            response = requests.options(url, auth=auth)
            if 'allow' in response.headers:
                methods = response.headers['allow']
                print(str(colored("[+] Allowed methods: ", 'green') + colored(methods, 'cyan')))
            else:
                print(colored("[!] Unable to determine allowed methods.", 'red'))
        except Exception as e:
            print(str(colored("[!] Error fetching options: ", 'green') + colored(e, 'cyan')))
            return
        
        if 'PROPFIND' in methods:
            headers = {
                'Depth': str(depth),
                'Content-Type': 'application/xml'
            }
            body = '<?xml version="1.0"?><a:propfind xmlns:a="DAV:"><a:allprop/></a:propfind>'
            try:
                response = requests.request('PROPFIND', url, headers=headers, data=body, auth=auth)
                if response.status_code == 207:
                    print_resources(response.content)
            except Exception as e:
                print(str(colored("[!] Error fetching resources with PROPFIND: ", 'red') + colored(e, 'yellow')))
    except:
        pass
    
def print_resources(response_content):
    ns = {'D': 'DAV:'}
    tree = ET.fromstring(response_content)
    for response in tree.findall('D:response', ns):
        href = response.find('D:href', ns)
        if href is not None:
            print(str(colored("[+] Resources: ", 'green') + colored(href.text, 'cyan')))

def net_info():
    print("\n")
    interfaces = psutil.net_if_addrs()
    for interfaz, info_addrs in interfaces.items():
        print(colored(f"Interface: {interfaz}", 'green'))
        mac_address = None
        for addr in info_addrs:
            if addr.family == socket.AF_INET:
                print(colored(f"  IP: {addr.address}", 'green'))
                print(colored(f"  Netmask: {addr.netmask}", 'green'))
                if addr.broadcast:
                    print(colored(f"  Broadcast: {addr.broadcast}", 'green'))
            elif addr.family == socket.AF_INET6:
                print(colored(f"  IPv6: {addr.address}", 'green'))

            if addr.family == psutil.AF_LINK:
                mac_address = addr.address
        if mac_address:
            print(colored(f"  MAC: {mac_address}", 'green'))
        print()
